{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f09498",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Neural Operator for Psilocybin Production Trajectory Prediction\n",
    "================================================================\n",
    "\n",
    "Fixed approach: Predict FUTURE timepoints only from early measurements + genetics\n",
    "Evaluate only on the future window where predictions matter\n",
    "\n",
    "Dataset: mltest.xlsx with 8 conditions (F503_C1, F503_C2, etc.)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: Load Data and Add Genetic Features\n",
    "# ============================================================================\n",
    "\n",
    "def load_data_with_genetics(filepath='../data/subs_omics_data/mltest.xlsx'):\n",
    "    \"\"\"Load data and add genetic feature encoding\"\"\"\n",
    "    \n",
    "    df = pd.read_excel(filepath)\n",
    "    df = df.rename(columns={\"Hours\": \"Time\"})\n",
    "    \n",
    "    # Extract strain/replicate\n",
    "    df['Strain'] = df['SampleID'].str.extract(r'(F\\d{2,3}_C\\d+)', flags=re.IGNORECASE)[0]\n",
    "    df['Replicate'] = df['SampleID'].str.extract(r'R(\\d+)')[0]\n",
    "    df = df.dropna(subset=['Psilocybin g/L'])\n",
    "    \n",
    "    # Genetic features\n",
    "    genetic_features = {\n",
    "        'F503_C1': {'ylAro1': 0, 'ylAro2': 0, 'ylAro4': 0, 'ylTrp2': 0, 'PcCpr': 0},\n",
    "        'F506_C1': {'ylAro1': 1, 'ylAro2': 1, 'ylAro4': 0, 'ylTrp2': 0, 'PcCpr': 0},\n",
    "        'F503_C2': {'ylAro1': 0, 'ylAro2': 0, 'ylAro4': 1, 'ylTrp2': 1, 'PcCpr': 0},\n",
    "        'F504_C1': {'ylAro1': 1, 'ylAro2': 1, 'ylAro4': 1, 'ylTrp2': 1, 'PcCpr': 0},\n",
    "        'F504_C2': {'ylAro1': 0, 'ylAro2': 0, 'ylAro4': 0, 'ylTrp2': 0, 'PcCpr': 1},\n",
    "        'F505_C2': {'ylAro1': 1, 'ylAro2': 1, 'ylAro4': 0, 'ylTrp2': 0, 'PcCpr': 1},\n",
    "        'F505_C1': {'ylAro1': 0, 'ylAro2': 0, 'ylAro4': 1, 'ylTrp2': 1, 'PcCpr': 1},\n",
    "        'F506_C2': {'ylAro1': 1, 'ylAro2': 1, 'ylAro4': 1, 'ylTrp2': 1, 'PcCpr': 1}\n",
    "    }\n",
    "    \n",
    "    for gene in ['ylAro1', 'ylAro2', 'ylAro4', 'ylTrp2', 'PcCpr']:\n",
    "        df[gene] = df['Strain'].map(lambda x: genetic_features.get(x, {}).get(gene, 0))\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"DATA LOADED WITH GENETIC FEATURES\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Conditions: {sorted(df['Strain'].dropna().unique())}\")\n",
    "    print(f\"Time range: {df['Time'].min():.1f}h to {df['Time'].max():.1f}h\")\n",
    "    \n",
    "    return df, genetic_features\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: Create Training Sequences (Fixed)\n",
    "# ============================================================================\n",
    "\n",
    "def create_training_sequences(df, early_cutoff=50):\n",
    "    \"\"\"\n",
    "    Create sequences that predict FUTURE timepoints from early measurements\n",
    "    Key fix: Store early and future separately, evaluate only on future\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"CREATING TRAINING SEQUENCES (FIXED)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Early window: 0-{early_cutoff}h (input)\")\n",
    "    print(f\"Future window: {early_cutoff}h+ (prediction target)\\n\")\n",
    "    \n",
    "    for strain in sorted(df['Strain'].dropna().unique()):\n",
    "        strain_data = df[df['Strain'] == strain].copy()\n",
    "        \n",
    "        for rep in sorted(strain_data['Replicate'].dropna().unique()):\n",
    "            run_data = strain_data[strain_data['Replicate'] == rep].sort_values('Time').reset_index(drop=True)\n",
    "            \n",
    "            if len(run_data) < 3:\n",
    "                continue\n",
    "            \n",
    "            # Split early and future\n",
    "            early_mask = run_data['Time'] <= early_cutoff\n",
    "            early_data = run_data[early_mask]\n",
    "            future_data = run_data[~early_mask]\n",
    "            \n",
    "            if len(early_data) < 1 or len(future_data) < 1:\n",
    "                continue\n",
    "            \n",
    "            # Genetic features\n",
    "            genetic_feats = run_data[['ylAro1', 'ylAro2', 'ylAro4', 'ylTrp2', 'PcCpr']].iloc[0].values\n",
    "            \n",
    "            # Early features\n",
    "            early_features = np.concatenate([\n",
    "                [early_data['Time'].max()],\n",
    "                [early_data['Biomass (g/L)'].mean()],\n",
    "                [early_data['Biomass (g/L)'].max()],\n",
    "                [early_data['OD600 (N/A)'].mean()],\n",
    "                [early_data['Psilocin g/L'].mean()],\n",
    "                [early_data['Psilocin g/L'].max()],\n",
    "                [early_data['Baeocystine g/L'].mean()],\n",
    "                [early_data['Tryptamine g/L'].mean()],\n",
    "                [early_data['Psilocybin g/L'].mean()],\n",
    "                [early_data['Psilocybin g/L'].max()],\n",
    "                genetic_feats\n",
    "            ])\n",
    "            \n",
    "            sequences.append({\n",
    "                'strain': strain,\n",
    "                'replicate': rep,\n",
    "                'features': early_features,\n",
    "                'early_time': early_data['Time'].values,\n",
    "                'early_psilocybin': early_data['Psilocybin g/L'].values,\n",
    "                'future_time': future_data['Time'].values,\n",
    "                'future_psilocybin': future_data['Psilocybin g/L'].values,\n",
    "                'full_time': run_data['Time'].values,\n",
    "                'full_psilocybin': run_data['Psilocybin g/L'].values,\n",
    "            })\n",
    "    \n",
    "    print(f\"âœ“ Created {len(sequences)} sequences\")\n",
    "    print(f\"  Average early points: {np.mean([len(s['early_time']) for s in sequences]):.1f}\")\n",
    "    print(f\"  Average future points: {np.mean([len(s['future_time']) for s in sequences]):.1f}\")\n",
    "    \n",
    "    from collections import Counter\n",
    "    strain_counts = Counter(s['strain'] for s in sequences)\n",
    "    print(f\"\\nSequences per condition:\")\n",
    "    for strain in sorted(strain_counts.keys()):\n",
    "        print(f\"  {strain}: {strain_counts[strain]} replicates\")\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: Neural Operator Model (Variable Length Output)\n",
    "# ============================================================================\n",
    "\n",
    "class GeneticTrajectoryPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Predicts multiple future timepoints from early measurements + genetics\n",
    "    Outputs a reasonable number of future predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features=15, n_future_points=20):\n",
    "        super().__init__()\n",
    "        self.n_future_points = n_future_points\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_features, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, n_future_points),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        future_trajectory = self.decoder(latent)\n",
    "        return torch.relu(future_trajectory)\n",
    "\n",
    "\n",
    "def prepare_data_for_future_prediction(sequences, n_future_points=20):\n",
    "    \"\"\"\n",
    "    Prepare data: early features â†’ future trajectory (interpolated to fixed length)\n",
    "    \"\"\"\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    for seq in sequences:\n",
    "        X_list.append(seq['features'])\n",
    "        \n",
    "        # Interpolate future trajectory to n_future_points\n",
    "        future_time = seq['future_time']\n",
    "        future_vals = seq['future_psilocybin']\n",
    "        \n",
    "        if len(future_time) > 0:\n",
    "            # Create uniform time grid for future\n",
    "            time_grid = np.linspace(future_time.min(), future_time.max(), n_future_points)\n",
    "            # Interpolate\n",
    "            interpolated = np.interp(time_grid, future_time, future_vals)\n",
    "            y_list.append(interpolated)\n",
    "        else:\n",
    "            y_list.append(np.zeros(n_future_points))\n",
    "    \n",
    "    X = torch.FloatTensor(np.array(X_list))\n",
    "    y = torch.FloatTensor(np.array(y_list))\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_normalized = scaler.fit_transform(X.numpy())\n",
    "    X = torch.FloatTensor(X_normalized)\n",
    "    \n",
    "    return X, y, scaler\n",
    "\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, n_epochs=200):\n",
    "    \"\"\"Train neural operator\"\"\"\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=25, factor=0.5)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred_train = model(X_train)\n",
    "        loss_train = criterion(pred_train, y_train)\n",
    "        loss_train.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_val = model(X_val)\n",
    "            loss_val = criterion(pred_val, y_val)\n",
    "        \n",
    "        scheduler.step(loss_val)\n",
    "        train_losses.append(loss_train.item())\n",
    "        val_losses.append(loss_val.item())\n",
    "        \n",
    "        if loss_val < best_val_loss:\n",
    "            best_val_loss = loss_val\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= 60:\n",
    "            if epoch > 20:  # Allow at least 20 epochs\n",
    "                break\n",
    "    \n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, train_losses, val_losses, best_val_loss\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 4: Leave-One-Condition-Out CV (Fixed Evaluation)\n",
    "# ============================================================================\n",
    "\n",
    "def leave_one_condition_out_cv(sequences, n_future_points=20):\n",
    "    \"\"\"\n",
    "    Cross-validation with proper evaluation on future timepoints only\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LEAVE-ONE-CONDITION-OUT CROSS-VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    conditions = sorted(set(s['strain'] for s in sequences))\n",
    "    results = []\n",
    "    \n",
    "    for test_condition in conditions:\n",
    "        print(f\"\\nHolding out: {test_condition}\")\n",
    "        \n",
    "        train_seqs = [s for s in sequences if s['strain'] != test_condition]\n",
    "        test_seqs = [s for s in sequences if s['strain'] == test_condition]\n",
    "        \n",
    "        X_train, y_train, scaler = prepare_data_for_future_prediction(train_seqs, n_future_points)\n",
    "        X_test, y_test, _ = prepare_data_for_future_prediction(test_seqs, n_future_points)\n",
    "        X_test = torch.FloatTensor(scaler.transform(X_test.numpy()))\n",
    "        \n",
    "        # Train\n",
    "        model = GeneticTrajectoryPredictor(n_features=X_train.shape[1], n_future_points=n_future_points)\n",
    "        model, _, _, best_val_loss = train_model(model, X_train, y_train, X_test, y_test, n_epochs=100)\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model(X_test).numpy()\n",
    "        \n",
    "        # Calculate metrics on future predictions\n",
    "        r2 = r2_score(y_test.numpy().flatten(), preds.flatten())\n",
    "        rmse = np.sqrt(mean_squared_error(y_test.numpy().flatten(), preds.flatten()))\n",
    "        \n",
    "        results.append({\n",
    "            'condition': test_condition,\n",
    "            'r2': r2,\n",
    "            'rmse': rmse,\n",
    "            'n_test': len(test_seqs),\n",
    "            'val_loss': best_val_loss\n",
    "        })\n",
    "        \n",
    "        print(f\"  RÂ² = {r2:.4f}, RMSE = {rmse:.4f} g/L\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"CROSS-VALIDATION SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(results_df.to_string(index=False))\n",
    "    print(f\"\\nMean RÂ²: {results_df['r2'].mean():.4f} (+/- {results_df['r2'].std():.4f})\")\n",
    "    print(f\"Mean RMSE: {results_df['rmse'].mean():.4f} g/L\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 5: Visualization\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_predictions_by_condition(df, model, scaler, sequences, early_cutoff=50, n_future_points=20):\n",
    "    \"\"\"\n",
    "    Visualize predictions for all 8 conditions\n",
    "    Shows early data + predicted future trajectory\n",
    "    \"\"\"\n",
    "    \n",
    "    conditions = sorted(df['Strain'].dropna().unique())\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    fig.suptitle('Neural Operator: Psilocybin Trajectory Predictions with Genetic Features', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, condition in enumerate(conditions):\n",
    "        ax = axes[idx // 4, idx % 4]\n",
    "        \n",
    "        condition_seqs = [s for s in sequences if s['strain'] == condition]\n",
    "        \n",
    "        if len(condition_seqs) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Plot all replicate data\n",
    "        for seq in condition_seqs:\n",
    "            ax.scatter(seq['full_time'], seq['full_psilocybin'], \n",
    "                      c='red', s=50, alpha=0.6, zorder=10, edgecolors='black')\n",
    "        \n",
    "        # Get predictions\n",
    "        X_cond = torch.FloatTensor(np.array([s['features'] for s in condition_seqs]))\n",
    "        X_cond = torch.FloatTensor(scaler.transform(X_cond.numpy()))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            future_preds = model(X_cond).numpy()\n",
    "        \n",
    "        # Create time axis for future predictions\n",
    "        max_future_time = max(s['future_time'].max() if len(s['future_time']) > 0 else early_cutoff \n",
    "                             for s in condition_seqs)\n",
    "        future_time_axis = np.linspace(early_cutoff, max_future_time, n_future_points)\n",
    "        \n",
    "        # Average predictions\n",
    "        mean_pred = future_preds.mean(axis=0)\n",
    "        std_pred = future_preds.std(axis=0)\n",
    "        \n",
    "        # Plot prediction\n",
    "        ax.plot(future_time_axis, mean_pred, 'b-', lw=3, label='Neural Op', zorder=5)\n",
    "        ax.fill_between(future_time_axis, \n",
    "                        np.maximum(mean_pred - 1.96 * std_pred, 0),\n",
    "                        mean_pred + 1.96 * std_pred,\n",
    "                        alpha=0.3, color='blue', zorder=4)\n",
    "        \n",
    "        # Mark early cutoff\n",
    "        ax.axvline(x=early_cutoff, color='gray', linestyle='--', alpha=0.5, linewidth=1.5,\n",
    "                  label=f'Early cutoff ({early_cutoff}h)')\n",
    "        \n",
    "        ax.set_xlabel('Time (hours)', fontsize=10)\n",
    "        ax.set_ylabel('Psilocybin (g/L)', fontsize=10)\n",
    "        ax.set_title(condition, fontsize=12, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('neural_operator_predictions_fixed.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nâœ“ Figure: Neural operator predictions (8 conditions)\")\n",
    "\n",
    "\n",
    "def compare_to_baseline(sequences, early_cutoff=50):\n",
    "    \"\"\"Compare to simple baselines on FUTURE predictions only\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BASELINE COMPARISON (Future Predictions Only)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    baselines = {'last_value': [], 'linear': []}\n",
    "    \n",
    "    for seq in sequences:\n",
    "        if len(seq['future_time']) < 1 or len(seq['early_time']) < 1:\n",
    "            continue\n",
    "        \n",
    "        early_vals = seq['early_psilocybin']\n",
    "        future_actual = seq['future_psilocybin']\n",
    "        future_time = seq['future_time']\n",
    "        early_time = seq['early_time']\n",
    "        \n",
    "        # Baseline 1: Last value forward\n",
    "        last_val = early_vals[-1]\n",
    "        pred_last = np.full(len(future_actual), last_val)\n",
    "        r2_last = r2_score(future_actual, pred_last)\n",
    "        baselines['last_value'].append(r2_last)\n",
    "        \n",
    "        # Baseline 2: Linear extrapolation\n",
    "        if len(early_vals) >= 2:\n",
    "            slope = (early_vals[-1] - early_vals[0]) / (early_time[-1] - early_time[0])\n",
    "            pred_linear = early_vals[-1] + slope * (future_time - early_time[-1])\n",
    "            pred_linear = np.maximum(pred_linear, 0)\n",
    "            r2_linear = r2_score(future_actual, pred_linear)\n",
    "            baselines['linear'].append(r2_linear)\n",
    "    \n",
    "    print(f\"\\nBaseline RÂ² (future predictions only):\")\n",
    "    print(f\"  Last Value Forward: {np.mean(baselines['last_value']):.4f} (+/- {np.std(baselines['last_value']):.4f})\")\n",
    "    if len(baselines['linear']) > 0:\n",
    "        print(f\"  Linear Extrapolation: {np.mean(baselines['linear']):.4f} (+/- {np.std(baselines['linear']):.4f})\")\n",
    "    \n",
    "    return baselines\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 6: Main Workflow\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Complete workflow with fixed evaluation\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"NEURAL OPERATOR WITH GENETIC FEATURES (FIXED)\")\n",
    "    print(\"Predicting FUTURE trajectories from early measurements\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load data\n",
    "    df, genetic_features = load_data_with_genetics()\n",
    "    \n",
    "    # Create sequences\n",
    "    sequences = create_training_sequences(df, early_cutoff=50)\n",
    "    \n",
    "    # Baseline comparison\n",
    "    baselines = compare_to_baseline(sequences, early_cutoff=50)\n",
    "    \n",
    "    # Train a full model for visualization\n",
    "    n_future_points = 20\n",
    "    X_all, y_all, scaler = prepare_data_for_future_prediction(sequences, n_future_points)\n",
    "    \n",
    "    # Simple 80/20 split for demo model\n",
    "    n_train = int(0.8 * len(X_all))\n",
    "    indices = np.random.RandomState(42).permutation(len(X_all))\n",
    "    train_idx = indices[:n_train]\n",
    "    val_idx = indices[n_train:]\n",
    "    \n",
    "    X_train, X_val = X_all[train_idx], X_all[val_idx]\n",
    "    y_train, y_val = y_all[train_idx], y_all[val_idx]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TRAINING FULL MODEL FOR VISUALIZATION\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Train: {len(train_idx)} sequences, Val: {len(val_idx)} sequences\")\n",
    "    \n",
    "    model = GeneticTrajectoryPredictor(n_features=X_all.shape[1], n_future_points=n_future_points)\n",
    "    model, train_losses, val_losses, best_val_loss = train_model(\n",
    "        model, X_train, y_train, X_val, y_val, n_epochs=200\n",
    "    )\n",
    "    print(f\"âœ“ Training complete! Best val loss: {best_val_loss:.6f}\")\n",
    "    \n",
    "    # Visualize\n",
    "    visualize_predictions_by_condition(df, model, scaler, sequences, early_cutoff=50, n_future_points=n_future_points)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_results = leave_one_condition_out_cv(sequences, n_future_points=n_future_points)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUCCESS METRICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nâœ“ Model trained on 8 conditions with genetic features\")\n",
    "    print(f\"âœ“ Leave-one-condition-out RÂ²: {cv_results['r2'].mean():.4f}\")\n",
    "    print(f\"âœ“ Baseline (last-value): {np.mean(baselines['last_value']):.4f}\")\n",
    "    if len(baselines['linear']) > 0:\n",
    "        print(f\"âœ“ Baseline (linear): {np.mean(baselines['linear']):.4f}\")\n",
    "    \n",
    "    if cv_results['r2'].mean() > 0:\n",
    "        print(f\"\\nðŸŽ‰ SUCCESS: Model learns trajectory dynamics (RÂ² > 0)\")\n",
    "    if cv_results['r2'].mean() > np.mean(baselines['last_value']):\n",
    "        print(f\"ðŸŽ‰ SUCCESS: Beats simple baselines!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return model, scaler, sequences, cv_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, scaler, sequences, cv_results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b328ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def visualize_ensemble_predictions(df, sequences, scaler, n_future_points=20, n_ensembles=3):\n",
    "    \"\"\"\n",
    "    Visualize ensemble predictions per strain:\n",
    "    - plots early data, actual future data, and ensemble mean Â± CI\n",
    "    - uses the new Î”-trajectory formulation (adds back last early value)\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import r2_score, mean_squared_error\n",
    "    \n",
    "    strains = sorted(df['Strain'].dropna().unique())\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    fig.suptitle(\"Neural Operator Ensemble (Î”-Trajectory) Predictions\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    for idx, strain in enumerate(strains):\n",
    "        ax = axes[idx // 4, idx % 4]\n",
    "        strain_seqs = [s for s in sequences if s['strain'] == strain]\n",
    "        if len(strain_seqs) == 0:\n",
    "            continue\n",
    "\n",
    "        # Prepare data for this strain\n",
    "        X_test = torch.FloatTensor(np.array([s['features'] for s in strain_seqs]))\n",
    "        X_test = torch.FloatTensor(scaler.transform(X_test.numpy()))\n",
    "        last_vals = np.array([s['early_psilocybin'][-1] for s in strain_seqs])[:, None]\n",
    "\n",
    "        # Collect ensemble predictions\n",
    "        preds = []\n",
    "        for _ in range(n_ensembles):\n",
    "            model = SmallNeuralOp(n_features=X_test.shape[1], n_future_points=n_future_points)\n",
    "            model = train_model(model, X_test, torch.zeros_like(torch.rand(X_test.size(0), n_future_points)),\n",
    "                                X_test, torch.zeros_like(torch.rand(X_test.size(0), n_future_points)))\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_delta = model(X_test).numpy()\n",
    "            preds.append(pred_delta + last_vals)\n",
    "\n",
    "        preds = np.stack(preds)  # (n_ensembles, n_samples, n_future_points)\n",
    "        mean_pred = preds.mean(axis=0).mean(axis=0)\n",
    "        std_pred = preds.std(axis=0).mean(axis=0)\n",
    "\n",
    "        # Build time axis\n",
    "        max_future_time = max(s['future_time'].max() if len(s['future_time']) > 0 else 0 for s in strain_seqs)\n",
    "        min_future_time = min(s['future_time'].min() if len(s['future_time']) > 0 else 50 for s in strain_seqs)\n",
    "        time_axis = np.linspace(min_future_time, max_future_time, n_future_points)\n",
    "\n",
    "        # Plot actual data\n",
    "        for s in strain_seqs:\n",
    "            ax.scatter(s['early_time'], s['early_psilocybin'], c='gray', s=50, alpha=0.6, label='Early' if idx == 0 else \"\")\n",
    "            ax.scatter(s['future_time'], s['future_psilocybin'], c='red', s=50, alpha=0.8, edgecolors='black', label='Actual' if idx == 0 else \"\")\n",
    "\n",
    "        # Plot ensemble mean prediction with CI\n",
    "        ax.plot(time_axis, mean_pred, 'b-', lw=3, label='Predicted mean' if idx == 0 else \"\")\n",
    "        ax.fill_between(time_axis,\n",
    "                        np.maximum(mean_pred - 1.96 * std_pred, 0),\n",
    "                        mean_pred + 1.96 * std_pred,\n",
    "                        color='blue', alpha=0.25, label='95% CI' if idx == 0 else \"\")\n",
    "\n",
    "        ax.axvline(x=50, color='gray', linestyle='--', alpha=0.6)\n",
    "        ax.set_title(strain, fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Time (hours)', fontsize=10)\n",
    "        ax.set_ylabel('Psilocybin (g/L)', fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        if idx == 0:\n",
    "            ax.legend(fontsize=9, loc='best')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371701fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Full Workflow: Pretrain FNO on Simulated Psilocybin Trajectories + Fine-tune on Real Data\n",
    "========================================================================================\n",
    "\n",
    "Features:\n",
    "- Neural Operator (FNO) learns functional mapping from early trajectory + genetics â†’ future trajectory\n",
    "- Pretraining on simulated data to improve generalization\n",
    "- Fine-tuning on real experimental conditions\n",
    "- Leave-One-Condition-Out CV and visualization\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ============================================================================ #\n",
    "# PART 0: Simulated Psilocybin Trajectories for Pretraining\n",
    "# ============================================================================ #\n",
    "\n",
    "def generate_simulated_trajectories(n_samples=500, n_future_points=20, seed=42):\n",
    "    \"\"\"Generate smooth, varied psilocybin trajectories for pretraining\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    X_sim = []\n",
    "    y_sim = []\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        # Randomized early features\n",
    "        biomass_init = np.random.uniform(0.1, 1.0)\n",
    "        od600_init = np.random.uniform(0.1, 1.0)\n",
    "        psilocybin_init = np.random.uniform(0.0, 0.1)\n",
    "        genetic_feats = np.random.randint(0, 2, size=5)\n",
    "\n",
    "        features = np.array([\n",
    "            biomass_init,\n",
    "            od600_init,\n",
    "            psilocybin_init,\n",
    "            np.mean([biomass_init, od600_init]),\n",
    "            np.max([biomass_init, od600_init]),\n",
    "            *genetic_feats\n",
    "        ])\n",
    "        X_sim.append(features)\n",
    "\n",
    "        # Simulated trajectory: logistic growth + noise\n",
    "        t = np.linspace(0, 100, n_future_points)\n",
    "        r = np.random.uniform(0.05, 0.15)\n",
    "        K = np.random.uniform(1.0, 2.0)\n",
    "        y = K / (1 + ((K - psilocybin_init)/psilocybin_init) * np.exp(-r*t))\n",
    "        y += np.random.normal(0, 0.02, size=n_future_points)  # small noise\n",
    "        y_sim.append(y.clip(0))\n",
    "\n",
    "    X_sim = torch.FloatTensor(np.array(X_sim))\n",
    "    y_sim = torch.FloatTensor(np.array(y_sim))\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_sim = torch.FloatTensor(scaler.fit_transform(X_sim.numpy()))\n",
    "\n",
    "    return X_sim, y_sim, scaler\n",
    "\n",
    "\n",
    "# ============================================================================ #\n",
    "# PART 1: Fourier Neural Operator\n",
    "# ============================================================================ #\n",
    "\n",
    "class FourierBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_modes=8):\n",
    "        super().__init__()\n",
    "        self.n_modes = n_modes\n",
    "        scale = 1 / (in_channels * out_channels)\n",
    "        self.weights = nn.Parameter(scale * torch.randn(in_channels, out_channels, n_modes, dtype=torch.cfloat))\n",
    "\n",
    "    def compl_mul1d(self, input, weights):\n",
    "        return torch.einsum(\"bim, iom -> bom\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, L = x.shape\n",
    "        x_ft = torch.fft.rfft(x)\n",
    "        out_ft = torch.zeros(B, self.weights.shape[1], x_ft.size(-1), dtype=torch.cfloat, device=x.device)\n",
    "        out_ft[:, :, :self.n_modes] = self.compl_mul1d(x_ft[:, :, :self.n_modes], self.weights)\n",
    "        x = torch.fft.irfft(out_ft, n=L)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FNO1D(nn.Module):\n",
    "    \"\"\"Fourier Neural Operator mapping early features â†’ future trajectory\"\"\"\n",
    "    def __init__(self, n_features=11, n_future_points=20, width=64, n_modes=8, depth=4):\n",
    "        super().__init__()\n",
    "        self.n_future_points = n_future_points\n",
    "        self.fc0 = nn.Linear(n_features, width)\n",
    "        self.fourier_layers = nn.ModuleList([FourierBlock1D(width, width, n_modes) for _ in range(depth)])\n",
    "        self.ws = nn.ModuleList([nn.Conv1d(width, width, 1) for _ in range(depth)])\n",
    "        self.fc1 = nn.Linear(width, 128)\n",
    "        self.fc2 = nn.Linear(128, n_future_points)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1).repeat(1, 1, self.n_future_points)  # (B, n_features, L)\n",
    "        x = self.fc0(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        for fourier, w in zip(self.fourier_layers, self.ws):\n",
    "            x = fourier(x) + w(x)\n",
    "            x = torch.relu(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        out = torch.relu(self.fc2(x))\n",
    "        out = out.mean(dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ============================================================================ #\n",
    "# PART 2: Training Function\n",
    "# ============================================================================ #\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val=None, y_val=None, n_epochs=200, lr=1e-3):\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred_train = model(X_train)\n",
    "        loss_train = criterion(pred_train, y_train)\n",
    "        loss_train.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss_train.item())\n",
    "\n",
    "        if X_val is not None and y_val is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_val = model(X_val)\n",
    "                loss_val = criterion(pred_val, y_val)\n",
    "            val_losses.append(loss_val.item())\n",
    "            if loss_val < best_val_loss:\n",
    "                best_val_loss = loss_val\n",
    "                best_model_state = model.state_dict().copy()\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "\n",
    "# ============================================================================ #\n",
    "# PART 3: Fine-tuning on Real Dataset\n",
    "# ============================================================================ #\n",
    "\n",
    "def load_real_data(filepath='../data/subs_omics_data/mltest.xlsx', early_cutoff=50):\n",
    "    df = pd.read_excel(filepath)\n",
    "    df = df.rename(columns={\"Hours\": \"Time\"})\n",
    "    df['Strain'] = df['SampleID'].str.extract(r'(F\\d{2,3}_C\\d+)')[0]\n",
    "    df['Replicate'] = df['SampleID'].str.extract(r'R(\\d+)')[0]\n",
    "    df = df.dropna(subset=['Psilocybin g/L'])\n",
    "\n",
    "    genetic_features = {\n",
    "        'F503_C1': [0,0,0,0,0],\n",
    "        'F506_C1': [1,1,0,0,0],\n",
    "        'F503_C2': [0,0,1,1,0],\n",
    "        'F504_C1': [1,1,1,1,0],\n",
    "        'F504_C2': [0,0,0,0,1],\n",
    "        'F505_C2': [1,1,0,0,1],\n",
    "        'F505_C1': [0,0,1,1,1],\n",
    "        'F506_C2': [1,1,1,1,1]\n",
    "    }\n",
    "\n",
    "    sequences = []\n",
    "    for strain in df['Strain'].dropna().unique():\n",
    "        strain_data = df[df['Strain'] == strain]\n",
    "        for rep in strain_data['Replicate'].dropna().unique():\n",
    "            run_data = strain_data[strain_data['Replicate']==rep].sort_values('Time')\n",
    "            early_mask = run_data['Time'] <= early_cutoff\n",
    "            early = run_data[early_mask]\n",
    "            future = run_data[~early_mask]\n",
    "            if len(early) < 1 or len(future) < 1: \n",
    "                continue\n",
    "            feats = np.concatenate([\n",
    "                [early['Biomass (g/L)'].mean(),\n",
    "                 early['OD600 (N/A)'].mean(),\n",
    "                 early['Psilocybin g/L'].mean()],\n",
    "                genetic_features[strain]\n",
    "            ])\n",
    "            sequences.append({\n",
    "                'features': feats,\n",
    "                'future_psilocybin': future['Psilocybin g/L'].values\n",
    "            })\n",
    "\n",
    "    # Prepare tensors\n",
    "    n_future_points = max(len(s['future_psilocybin']) for s in sequences)\n",
    "    X_real = torch.FloatTensor(np.array([s['features'] for s in sequences]))\n",
    "    y_real = torch.FloatTensor(np.array([np.pad(s['future_psilocybin'], (0, n_future_points - len(s['future_psilocybin'])), 'edge') for s in sequences]))\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_real = torch.FloatTensor(scaler.fit_transform(X_real.numpy()))\n",
    "\n",
    "    return X_real, y_real, scaler, sequences\n",
    "\n",
    "\n",
    "# ============================================================================ #\n",
    "# PART 4: Visualization\n",
    "# ============================================================================ #\n",
    "\n",
    "def plot_predictions(sequences, model, scaler, n_future_points=20, title='Predictions'):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    for s in sequences:\n",
    "        X_feat = torch.FloatTensor(scaler.transform(s['features'].reshape(1,-1)))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(X_feat).numpy().flatten()\n",
    "        plt.plot(range(n_future_points), pred, 'b-', alpha=0.6)\n",
    "        plt.plot(range(len(s['future_psilocybin'])), s['future_psilocybin'], 'r--', alpha=0.8)\n",
    "    plt.xlabel('Future time points')\n",
    "    plt.ylabel('Psilocybin g/L')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================================ #\n",
    "# PART 5: Main Workflow\n",
    "# ============================================================================ #\n",
    "\n",
    "def main():\n",
    "    # 1ï¸âƒ£ Pretrain on simulated data\n",
    "    X_sim, y_sim, sim_scaler = generate_simulated_trajectories(n_samples=500)\n",
    "    fno_model = FNO1D(n_features=X_sim.shape[1], n_future_points=y_sim.shape[1])\n",
    "    fno_model, train_losses, val_losses = train_model(fno_model, X_sim, y_sim, n_epochs=200, lr=1e-3)\n",
    "    print(\"âœ“ Pretraining on simulated data complete!\")\n",
    "\n",
    "    # 2ï¸âƒ£ Fine-tune on real dataset\n",
    "    X_real, y_real, real_scaler, sequences = load_real_data()\n",
    "    fno_model, train_losses, val_losses = train_model(fno_model, X_real, y_real, n_epochs=150, lr=5e-4)\n",
    "    print(\"âœ“ Fine-tuning on real data complete!\")\n",
    "\n",
    "    # 3ï¸âƒ£ Visualize predictions\n",
    "    plot_predictions(sequences, fno_model, real_scaler, title='FNO Predictions on Real Conditions')\n",
    "\n",
    "    # 4ï¸âƒ£ Evaluate RÂ² / RMSE\n",
    "    fno_model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = fno_model(X_real).numpy()\n",
    "    y_true = y_real.numpy()\n",
    "    r2 = r2_score(y_true.flatten(), preds.flatten())\n",
    "    rmse = np.sqrt(mean_squared_error(y_true.flatten(), preds.flatten()))\n",
    "    print(f\"\\nEvaluation: RÂ²={r2:.4f}, RMSE={rmse:.4f} g/L\")\n",
    "\n",
    "    return fno_model, real_scaler, sequences\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, scaler, sequences = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9bf258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================ #\n",
    "# PART 6: LOCO Cross-Validation + Uncertainty Visualization\n",
    "# ============================================================================ #\n",
    "\n",
    "def leave_one_condition_out_fno(sequences, model_class=FNO1D, n_future_points=20, n_epochs=150):\n",
    "    \"\"\"\n",
    "    Perform Leave-One-Condition-Out CV with FNO.\n",
    "    Returns metrics and plots.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import defaultdict\n",
    "\n",
    "    conditions = sorted(set(seq['features'][-5:].tolist() for seq in sequences))  # genetic features as key\n",
    "    results = []\n",
    "    \n",
    "    # Map genetic features â†’ strain name\n",
    "    strain_map = {tuple(seq['features'][-5:]): seq for seq in sequences}\n",
    "\n",
    "    plt.figure(figsize=(16,8))\n",
    "    \n",
    "    for idx, seq in enumerate(sequences):\n",
    "        # Identify condition to hold out\n",
    "        holdout_feat = seq['features'][-5:]\n",
    "        X_train = torch.FloatTensor(np.array([s['features'] for s in sequences if not np.array_equal(s['features'][-5:], holdout_feat)]))\n",
    "        y_train = torch.FloatTensor(np.array([np.pad(s['future_psilocybin'], (0, n_future_points - len(s['future_psilocybin'])), 'edge') \n",
    "                                              for s in sequences if not np.array_equal(s['features'][-5:], holdout_feat)]))\n",
    "        X_test = torch.FloatTensor(seq['features'].reshape(1,-1))\n",
    "        y_test = np.pad(seq['future_psilocybin'], (0, n_future_points - len(seq['future_psilocybin'])), 'edge')\n",
    "\n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train = torch.FloatTensor(scaler.fit_transform(X_train.numpy()))\n",
    "        X_test = torch.FloatTensor(scaler.transform(X_test.numpy()))\n",
    "\n",
    "        # Train FNO from scratch (or could use pretrained)\n",
    "        fno_model = model_class(n_features=X_train.shape[1], n_future_points=n_future_points)\n",
    "        fno_model, _, _ = train_model(fno_model, X_train, y_train, n_epochs=n_epochs)\n",
    "\n",
    "        # Predict\n",
    "        fno_model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = fno_model(X_test).numpy().flatten()\n",
    "\n",
    "        # Metrics\n",
    "        r2 = r2_score(y_test, pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "        results.append({'condition': idx, 'r2': r2, 'rmse': rmse})\n",
    "\n",
    "        # Plot\n",
    "        plt.plot(range(len(y_test)), pred, 'b-', alpha=0.6)\n",
    "        plt.plot(range(len(y_test)), y_test, 'r--', alpha=0.8)\n",
    "        plt.fill_between(range(len(y_test)), pred - 1.96*0.02, pred + 1.96*0.02, color='blue', alpha=0.2)  # approximate uncertainty\n",
    "\n",
    "    plt.xlabel('Future time points')\n",
    "    plt.ylabel('Psilocybin g/L')\n",
    "    plt.title('FNO LOCO-CV Predictions (Held-out Condition vs True)')\n",
    "    plt.show()\n",
    "\n",
    "    # Summary metrics\n",
    "    r2_mean = np.mean([r['r2'] for r in results])\n",
    "    rmse_mean = np.mean([r['rmse'] for r in results])\n",
    "    print(f\"LOCO-CV Mean RÂ²: {r2_mean:.4f}, Mean RMSE: {rmse_mean:.4f} g/L\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one just combines the last 2, maybe even the last 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421374f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Full Workflow: Fourier Neural Operator (FNO) for Psilocybin Trajectories\n",
    "========================================================================\n",
    "\n",
    "Steps:\n",
    "1. Pretrain FNO on simulated logistic-like trajectories (acts like PINN pretraining)\n",
    "2. Fine-tune on real dataset (8 conditions)\n",
    "3. Leave-One-Condition-Out CV with uncertainty visualization\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ============================================================================ #\n",
    "# PART 1: Generate Simulated Trajectories\n",
    "# ============================================================================ #\n",
    "\n",
    "def generate_simulated_trajectories(n_samples=500, n_future_points=20, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    X_sim, y_sim = [], []\n",
    "    for _ in range(n_samples):\n",
    "        biomass_init = np.random.uniform(0.1, 1.0)\n",
    "        od600_init = np.random.uniform(0.1, 1.0)\n",
    "        psilocybin_init = np.random.uniform(0.0, 0.1)\n",
    "        genetic_feats = np.random.randint(0, 2, size=5)\n",
    "        features = np.array([\n",
    "            biomass_init,\n",
    "            od600_init,\n",
    "            psilocybin_init,\n",
    "            np.mean([biomass_init, od600_init]),\n",
    "            np.max([biomass_init, od600_init]),\n",
    "            *genetic_feats\n",
    "        ])\n",
    "        X_sim.append(features)\n",
    "\n",
    "        # Logistic trajectory + small noise\n",
    "        t = np.linspace(0, 100, n_future_points)\n",
    "        r = np.random.uniform(0.05, 0.15)\n",
    "        K = np.random.uniform(1.0, 2.0)\n",
    "        y = K / (1 + ((K - psilocybin_init)/max(psilocybin_init, 1e-3)) * np.exp(-r*t))\n",
    "        y += np.random.normal(0, 0.02, size=n_future_points)\n",
    "        y_sim.append(y.clip(0))\n",
    "\n",
    "    X_sim = torch.FloatTensor(np.array(X_sim))\n",
    "    y_sim = torch.FloatTensor(np.array(y_sim))\n",
    "    scaler = StandardScaler()\n",
    "    X_sim = torch.FloatTensor(scaler.fit_transform(X_sim.numpy()))\n",
    "    return X_sim, y_sim, scaler\n",
    "\n",
    "\n",
    "# ============================================================================ #\n",
    "# PART 2: FNO Model Definition\n",
    "# ============================================================================ #\n",
    "\n",
    "class FourierBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_modes=8):\n",
    "        super().__init__()\n",
    "        self.n_modes = n_modes\n",
    "        scale = 1 / (in_channels * out_channels)\n",
    "        self.weights = nn.Parameter(scale * torch.randn(in_channels, out_channels, n_modes, dtype=torch.cfloat))\n",
    "\n",
    "    def compl_mul1d(self, input, weights):\n",
    "        return torch.einsum(\"bim, iom -> bom\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, L = x.shape\n",
    "        x_ft = torch.fft.rfft(x)\n",
    "        out_ft = torch.zeros(B, self.weights.shape[1], x_ft.size(-1), dtype=torch.cfloat, device=x.device)\n",
    "        out_ft[:, :, :self.n_modes] = self.compl_mul1d(x_ft[:, :, :self.n_modes], self.weights)\n",
    "        x = torch.fft.irfft(out_ft, n=L)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FNO1D(nn.Module):\n",
    "    def __init__(self, n_features=11, n_future_points=20, width=64, n_modes=8, depth=4):\n",
    "        super().__init__()\n",
    "        self.n_future_points = n_future_points\n",
    "        self.fc0 = nn.Linear(n_features, width)\n",
    "        self.fourier_layers = nn.ModuleList([FourierBlock1D(width, width, n_modes) for _ in range(depth)])\n",
    "        self.ws = nn.ModuleList([nn.Conv1d(width, width, 1) for _ in range(depth)])\n",
    "        self.fc1 = nn.Linear(width, 128)\n",
    "        self.fc2 = nn.Linear(128, n_future_points)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1).repeat(1, 1, self.n_future_points)\n",
    "        x = self.fc0(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        for fourier, w in zip(self.fourier_layers, self.ws):\n",
    "            x = fourier(x) + w(x)\n",
    "            x = torch.relu(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        out = torch.relu(self.fc2(x))\n",
    "        out = out.mean(dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ============================================================================ #\n",
    "# PART 3: Training\n",
    "# ============================================================================ #\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val=None, y_val=None, n_epochs=200, lr=1e-3):\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_train)\n",
    "        loss = criterion(pred, y_train)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        if X_val is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = criterion(model(X_val), y_val)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_state = model.state_dict().copy()\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================================================ #\n",
    "# PART 4: Real Data Loader\n",
    "# ============================================================================ #\n",
    "\n",
    "def load_real_data(filepath='../data/subs_omics_data/mltest.xlsx', early_cutoff=50):\n",
    "    df = pd.read_excel(filepath)\n",
    "    df = df.rename(columns={\"Hours\": \"Time\"})\n",
    "    df['Strain'] = df['SampleID'].str.extract(r'(F\\d{2,3}_C\\d+)')[0]\n",
    "    df['Replicate'] = df['SampleID'].str.extract(r'R(\\d+)')[0]\n",
    "    df = df.dropna(subset=['Psilocybin g/L'])\n",
    "\n",
    "    genetic_features = {\n",
    "        'F503_C1': [0,0,0,0,0],\n",
    "        'F506_C1': [1,1,0,0,0],\n",
    "        'F503_C2': [0,0,1,1,0],\n",
    "        'F504_C1': [1,1,1,1,0],\n",
    "        'F504_C2': [0,0,0,0,1],\n",
    "        'F505_C2': [1,1,0,0,1],\n",
    "        'F505_C1': [0,0,1,1,1],\n",
    "        'F506_C2': [1,1,1,1,1]\n",
    "    }\n",
    "\n",
    "    sequences = []\n",
    "    for strain in df['Strain'].dropna().unique():\n",
    "        strain_data = df[df['Strain'] == strain]\n",
    "        for rep in strain_data['Replicate'].dropna().unique():\n",
    "            run = strain_data[strain_data['Replicate']==rep].sort_values('Time')\n",
    "            early_mask = run['Time'] <= early_cutoff\n",
    "            early = run[early_mask]\n",
    "            future = run[~early_mask]\n",
    "            if len(early)<1 or len(future)<1:\n",
    "                continue\n",
    "            feats = np.concatenate([\n",
    "                [early['Biomass (g/L)'].mean(),\n",
    "                 early['OD600 (N/A)'].mean(),\n",
    "                 early['Psilocybin g/L'].mean()],\n",
    "                genetic_features[strain]\n",
    "            ])\n",
    "            sequences.append({'strain': strain, 'features': feats, 'future_psilocybin': future['Psilocybin g/L'].values})\n",
    "\n",
    "    n_future_points = max(len(s['future_psilocybin']) for s in sequences)\n",
    "    X = torch.FloatTensor(np.array([s['features'] for s in sequences]))\n",
    "    y = torch.FloatTensor(np.array([\n",
    "        np.pad(s['future_psilocybin'], (0, n_future_points - len(s['future_psilocybin'])), 'edge') \n",
    "        for s in sequences]))\n",
    "    scaler = StandardScaler()\n",
    "    X = torch.FloatTensor(scaler.fit_transform(X.numpy()))\n",
    "    return X, y, scaler, sequences\n",
    "\n",
    "\n",
    "# ============================================================================ #\n",
    "# PART 5: LOCO Cross-Validation with Visualization\n",
    "# ============================================================================ #\n",
    "\n",
    "def leave_one_condition_out_cv(sequences, model_class=FNO1D, n_future_points=20, n_epochs=150):\n",
    "    conditions = sorted(set(s['strain'] for s in sequences))\n",
    "    results = []\n",
    "\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    for i, test_cond in enumerate(conditions):\n",
    "        train_seqs = [s for s in sequences if s['strain'] != test_cond]\n",
    "        test_seqs = [s for s in sequences if s['strain'] == test_cond]\n",
    "\n",
    "        X_train = torch.FloatTensor(np.array([s['features'] for s in train_seqs]))\n",
    "        y_train = torch.FloatTensor(np.array([\n",
    "            np.pad(s['future_psilocybin'], (0, n_future_points - len(s['future_psilocybin'])), 'edge')\n",
    "            for s in train_seqs]))\n",
    "        X_test = torch.FloatTensor(np.array([s['features'] for s in test_seqs]))\n",
    "        y_test = torch.FloatTensor(np.array([\n",
    "            np.pad(s['future_psilocybin'], (0, n_future_points - len(s['future_psilocybin'])), 'edge')\n",
    "            for s in test_seqs]))\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = torch.FloatTensor(scaler.fit_transform(X_train.numpy()))\n",
    "        X_test = torch.FloatTensor(scaler.transform(X_test.numpy()))\n",
    "\n",
    "        model = model_class(n_features=X_train.shape[1], n_future_points=n_future_points)\n",
    "        model = train_model(model, X_train, y_train, n_epochs=n_epochs, lr=5e-4)\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(X_test).numpy()\n",
    "        y_true = y_test.numpy()\n",
    "\n",
    "        r2 = r2_score(y_true.flatten(), preds.flatten())\n",
    "        rmse = np.sqrt(mean_squared_error(y_true.flatten(), preds.flatten()))\n",
    "        results.append({'strain': test_cond, 'r2': r2, 'rmse': rmse})\n",
    "\n",
    "        # Plot\n",
    "        ax = plt.subplot(2, 4, i+1)\n",
    "        mean_pred = preds.mean(axis=0)\n",
    "        std_pred = preds.std(axis=0)\n",
    "        ax.plot(range(len(mean_pred)), mean_pred, 'b-', lw=2, label='Predicted')\n",
    "        ax.fill_between(range(len(mean_pred)),\n",
    "                        np.maximum(mean_pred - 1.96 * std_pred, 0),\n",
    "                        mean_pred + 1.96 * std_pred,\n",
    "                        color='blue', alpha=0.2)\n",
    "        for y_seq in y_true:\n",
    "            ax.plot(range(len(y_seq)), y_seq, 'r--', lw=1.5, alpha=0.8)\n",
    "        ax.set_title(f\"{test_cond}\\nRÂ²={r2:.2f}, RMSE={rmse:.3f}\")\n",
    "        ax.set_xlabel('Future time')\n",
    "        ax.set_ylabel('Psilocybin (g/L)')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    plt.suptitle(\"LOCO-CV: FNO Predictions with Uncertainty Bands\", fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nLOCO-CV Summary:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "    print(f\"\\nMean RÂ²: {results_df['r2'].mean():.4f}, Mean RMSE: {results_df['rmse'].mean():.4f}\")\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# ============================================================================ #\n",
    "# PART 6: Main Workflow\n",
    "# ============================================================================ #\n",
    "\n",
    "def main():\n",
    "    print(\"\\n========== PRETRAINING ON SIMULATED TRAJECTORIES ==========\")\n",
    "    X_sim, y_sim, _ = generate_simulated_trajectories()\n",
    "    model = FNO1D(n_features=X_sim.shape[1], n_future_points=y_sim.shape[1])\n",
    "    model = train_model(model, X_sim, y_sim, n_epochs=200, lr=1e-3)\n",
    "    print(\"âœ“ Pretraining complete.\")\n",
    "\n",
    "    print(\"\\n========== FINE-TUNING ON REAL DATA ==========\")\n",
    "    X_real, y_real, scaler, sequences = load_real_data()\n",
    "    model = train_model(model, X_real, y_real, n_epochs=150, lr=5e-4)\n",
    "    print(\"âœ“ Fine-tuning complete.\")\n",
    "\n",
    "    print(\"\\n========== LOCO CROSS-VALIDATION ==========\")\n",
    "    results_df = leave_one_condition_out_cv(sequences, model_class=FNO1D)\n",
    "    return model, scaler, sequences, results_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, scaler, sequences, results = main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
