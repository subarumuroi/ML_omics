{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b176a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "    â•‘        HYPERPARAMETER TUNING & SUCCESS VALIDATION SUITE              â•‘\n",
      "    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "    This module provides:\n",
      "    1. Baseline predictors (last-value, linear extrapolation)\n",
      "    2. Hyperparameter search grid\n",
      "    3. Leave-one-strain-out cross-validation\n",
      "    4. Success criteria evaluation\n",
      "    5. Comprehensive visualization\n",
      "\n",
      "    Usage:\n",
      "    ------\n",
      "    from hyperparameter_tuning import complete_validation_pipeline\n",
      "\n",
      "    success_metrics = complete_validation_pipeline(\n",
      "        model, X_train, y_train, masks_train,\n",
      "        X_val, y_val, masks_val, sequences, val_idx, rf_r2\n",
      "    )\n",
      "\n",
      "    if success_metrics['success']:\n",
      "        print(\"ğŸ‰ Model validated - ready for deployment!\")\n",
      "    else:\n",
      "        print(\"Continue tuning...\")\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Hyperparameter Tuning & Validation Suite\n",
    "=========================================\n",
    "\n",
    "Goal: Find optimal model configuration and validate success criteria\n",
    "\n",
    "Success Criteria:\n",
    "1. Positive RÂ² score (model better than mean prediction)\n",
    "2. Trajectory predictions qualitatively match ground truth\n",
    "3. Model generalizes across strains (leave-one-strain-out CV)\n",
    "4. Better than naive baselines (linear extrapolation, last-value-forward)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: Baseline Models for Comparison\n",
    "# ============================================================================\n",
    "\n",
    "class NaiveBaselines:\n",
    "    \"\"\"\n",
    "    Simple baseline predictors to beat.\n",
    "    If neural operator can't beat these, something is wrong!\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def last_value_forward(early_data, target_length):\n",
    "        \"\"\"\n",
    "        Predict: future = last observed value (constant extrapolation)\n",
    "        \"\"\"\n",
    "        last_value = early_data[-1]\n",
    "        prediction = np.full(target_length, last_value)\n",
    "        return prediction\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear_extrapolation(early_data, early_time, target_time):\n",
    "        \"\"\"\n",
    "        Fit line to early data, extrapolate linearly\n",
    "        \"\"\"\n",
    "        if len(early_data) < 2:\n",
    "            return np.full(len(target_time), early_data[-1])\n",
    "        \n",
    "        # Fit line to early data\n",
    "        coeffs = np.polyfit(early_time, early_data, 1)\n",
    "        slope, intercept = coeffs\n",
    "        \n",
    "        # Predict\n",
    "        prediction = slope * target_time + intercept\n",
    "        return np.maximum(prediction, 0)  # Non-negative\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_trajectory(all_trajectories):\n",
    "        \"\"\"\n",
    "        Predict: average trajectory across all training samples\n",
    "        \"\"\"\n",
    "        return np.mean(all_trajectories, axis=0)\n",
    "\n",
    "\n",
    "def evaluate_baselines(sequences, early_cutoff=48):\n",
    "    \"\"\"\n",
    "    Evaluate naive baselines to set performance floor.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EVALUATING BASELINE PREDICTORS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results = {\n",
    "        'last_value': [],\n",
    "        'linear_extrap': [],\n",
    "        'mean_trajectory': []\n",
    "    }\n",
    "    \n",
    "    # Calculate mean trajectory\n",
    "    all_trajectories = [s['psilocybin'] for s in sequences]\n",
    "    max_len = max(len(t) for t in all_trajectories)\n",
    "    \n",
    "    # Pad trajectories to same length\n",
    "    padded_trajectories = []\n",
    "    for traj in all_trajectories:\n",
    "        if len(traj) < max_len:\n",
    "            padded = np.pad(traj, (0, max_len - len(traj)), mode='edge')\n",
    "        else:\n",
    "            padded = traj[:max_len]\n",
    "        padded_trajectories.append(padded)\n",
    "    \n",
    "    mean_traj = np.mean(padded_trajectories, axis=0)\n",
    "    \n",
    "    # Evaluate on each sequence\n",
    "    for seq in sequences:\n",
    "        time = seq['time']\n",
    "        actual = seq['psilocybin']\n",
    "        \n",
    "        early_mask = time <= early_cutoff\n",
    "        if early_mask.sum() < 2:\n",
    "            continue\n",
    "        \n",
    "        early_time = time[early_mask]\n",
    "        early_values = actual[early_mask]\n",
    "        \n",
    "        future_time = time[~early_mask]\n",
    "        future_actual = actual[~early_mask]\n",
    "        \n",
    "        if len(future_actual) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Baseline 1: Last value forward\n",
    "        pred_last = NaiveBaselines.last_value_forward(early_values, len(future_actual))\n",
    "        r2_last = r2_score(future_actual, pred_last)\n",
    "        results['last_value'].append(r2_last)\n",
    "        \n",
    "        # Baseline 2: Linear extrapolation\n",
    "        pred_linear = NaiveBaselines.linear_extrapolation(early_values, early_time, future_time)\n",
    "        r2_linear = r2_score(future_actual, pred_linear)\n",
    "        results['linear_extrap'].append(r2_linear)\n",
    "        \n",
    "        # Baseline 3: Mean trajectory\n",
    "        pred_mean = mean_traj[len(early_values):len(actual)]\n",
    "        if len(pred_mean) > len(future_actual):\n",
    "            pred_mean = pred_mean[:len(future_actual)]\n",
    "        elif len(pred_mean) < len(future_actual):\n",
    "            pred_mean = np.pad(pred_mean, (0, len(future_actual) - len(pred_mean)), mode='edge')\n",
    "        \n",
    "        r2_mean = r2_score(future_actual, pred_mean)\n",
    "        results['mean_trajectory'].append(r2_mean)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\nBaseline Performance (mean RÂ² on future predictions):\")\n",
    "    for method, scores in results.items():\n",
    "        mean_r2 = np.mean(scores)\n",
    "        print(f\"  {method:20s}: RÂ² = {mean_r2:6.3f}\")\n",
    "    \n",
    "    print(\"\\nâœ“ Neural operator must beat these simple baselines!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: Hyperparameter Search\n",
    "# ============================================================================\n",
    "\n",
    "def hyperparameter_search(X_train, y_train, masks_train, \n",
    "                         X_val, y_val, masks_val,\n",
    "                         param_grid=None):\n",
    "    \"\"\"\n",
    "    Grid search over hyperparameters.\n",
    "    \n",
    "    Key hyperparameters:\n",
    "    - hidden_dim: Model capacity\n",
    "    - n_layers: Model depth\n",
    "    - dropout: Regularization\n",
    "    - learning_rate: Training speed\n",
    "    - weight_decay: L2 regularization\n",
    "    \"\"\"\n",
    "    \n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'hidden_dim': [128, 256, 512],\n",
    "            'dropout': [0.2, 0.3, 0.4],\n",
    "            'lr': [1e-3, 5e-4, 1e-4],\n",
    "            'weight_decay': [1e-4, 1e-3, 1e-2]\n",
    "        }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"HYPERPARAMETER SEARCH\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nSearch space:\")\n",
    "    for param, values in param_grid.items():\n",
    "        print(f\"  {param}: {values}\")\n",
    "    \n",
    "    # Simplified search: try a few key combinations\n",
    "    configs = [\n",
    "        {'hidden_dim': 128, 'dropout': 0.3, 'lr': 1e-3, 'weight_decay': 1e-4},\n",
    "        {'hidden_dim': 256, 'dropout': 0.3, 'lr': 5e-4, 'weight_decay': 1e-4},\n",
    "        {'hidden_dim': 512, 'dropout': 0.4, 'lr': 1e-4, 'weight_decay': 1e-3},\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, config in enumerate(configs):\n",
    "        print(f\"\\n[Config {i+1}/{len(configs)}] {config}\")\n",
    "        \n",
    "        # Build model\n",
    "        from psilocybin_neural_op import TrajectoryPredictor\n",
    "        \n",
    "        model = TrajectoryPredictor(\n",
    "            n_features=X_train.shape[1],\n",
    "            max_trajectory_length=y_train.shape[1]\n",
    "        )\n",
    "        \n",
    "        # Quick training (50 epochs)\n",
    "        optimizer = Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "        \n",
    "        def masked_mse_loss(pred, target, mask):\n",
    "            squared_error = (pred - target) ** 2\n",
    "            masked_error = squared_error * mask\n",
    "            return masked_error.sum() / mask.sum()\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(50):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            pred_train = model(X_train)\n",
    "            loss_train = masked_mse_loss(pred_train, y_train, masks_train)\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_val = model(X_val)\n",
    "                loss_val = masked_mse_loss(pred_val, y_val, masks_val)\n",
    "            \n",
    "            if loss_val < best_val_loss:\n",
    "                best_val_loss = loss_val\n",
    "        \n",
    "        # Calculate RÂ²\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_val = model(X_val).numpy()\n",
    "        \n",
    "        valid_pred = pred_val[masks_val.numpy() > 0].flatten()\n",
    "        valid_true = y_val.numpy()[masks_val.numpy() > 0].flatten()\n",
    "        r2 = r2_score(valid_true, valid_pred)\n",
    "        \n",
    "        results.append({\n",
    "            'config': config,\n",
    "            'val_loss': best_val_loss.item(),\n",
    "            'r2': r2\n",
    "        })\n",
    "        \n",
    "        print(f\"  â†’ Val Loss: {best_val_loss:.6f}, RÂ²: {r2:.4f}\")\n",
    "    \n",
    "    # Find best\n",
    "    best_idx = np.argmax([r['r2'] for r in results])\n",
    "    best_config = results[best_idx]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BEST CONFIGURATION:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Config: {best_config['config']}\")\n",
    "    print(f\"Val Loss: {best_config['val_loss']:.6f}\")\n",
    "    print(f\"RÂ²: {best_config['r2']:.4f}\")\n",
    "    \n",
    "    return best_config, results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: Leave-One-Strain-Out Cross-Validation\n",
    "# ============================================================================\n",
    "\n",
    "def leave_one_strain_out_cv(sequences, feature_names):\n",
    "    \"\"\"\n",
    "    Test if model generalizes to unseen strains.\n",
    "    This is the REAL test of transfer learning capability.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LEAVE-ONE-STRAIN-OUT CROSS-VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get unique strains\n",
    "    strains = list(set(s['strain'] for s in sequences if s['strain'] is not None))\n",
    "    print(f\"\\nTesting on {len(strains)} strains...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for test_strain in strains:\n",
    "        print(f\"\\n[Held-out strain: {test_strain}]\")\n",
    "        \n",
    "        # Split by strain\n",
    "        train_seqs = [s for s in sequences if s['strain'] != test_strain]\n",
    "        test_seqs = [s for s in sequences if s['strain'] == test_strain]\n",
    "        \n",
    "        if len(test_seqs) == 0:\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Train sequences: {len(train_seqs)}\")\n",
    "        print(f\"  Test sequences: {len(test_seqs)}\")\n",
    "        \n",
    "        # Prepare data (simplified - you'd use your actual preparation function)\n",
    "        # This is a placeholder - integrate with your actual data prep\n",
    "        print(f\"  Training model...\")\n",
    "        \n",
    "        # Would train model here on train_seqs, test on test_seqs\n",
    "        # For now, just report structure\n",
    "        \n",
    "        results.append({\n",
    "            'test_strain': test_strain,\n",
    "            'n_train': len(train_seqs),\n",
    "            'n_test': len(test_seqs)\n",
    "        })\n",
    "    \n",
    "    print(\"\\nâœ“ Cross-validation structure validated\")\n",
    "    print(\"  Next: Implement full training loop per fold\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 4: Success Criteria Evaluation\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_success_criteria(model, X_val, y_val, masks_val, sequences, \n",
    "                             baseline_results, rf_r2):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation against success criteria.\n",
    "    \n",
    "    SUCCESS = Model is useful for trajectory prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUCCESS CRITERIA EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_val).numpy()\n",
    "    \n",
    "    # Extract valid predictions\n",
    "    valid_pred = predictions[masks_val.numpy() > 0].flatten()\n",
    "    valid_true = y_val.numpy()[masks_val.numpy() > 0].flatten()\n",
    "    \n",
    "    # Metrics\n",
    "    r2 = r2_score(valid_true, valid_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(valid_true, valid_pred))\n",
    "    mae = np.mean(np.abs(valid_true - valid_pred))\n",
    "    \n",
    "    # Success criteria\n",
    "    criteria = {\n",
    "        'positive_r2': r2 > 0,\n",
    "        'beats_last_value': r2 > np.mean(baseline_results['last_value']),\n",
    "        'beats_linear': r2 > np.mean(baseline_results['linear_extrap']),\n",
    "        'reasonable_rmse': rmse < 0.2,  # Less than 0.2 g/L error\n",
    "        'low_mae': mae < 0.15  # Less than 0.15 g/L average error\n",
    "    }\n",
    "    \n",
    "    print(\"\\nğŸ“Š PERFORMANCE METRICS:\")\n",
    "    print(f\"  RÂ² Score:  {r2:.4f}\")\n",
    "    print(f\"  RMSE:      {rmse:.4f} g/L\")\n",
    "    print(f\"  MAE:       {mae:.4f} g/L\")\n",
    "    \n",
    "    print(\"\\nâœ… SUCCESS CRITERIA:\")\n",
    "    passed = 0\n",
    "    total = len(criteria)\n",
    "    \n",
    "    for criterion, passed_test in criteria.items():\n",
    "        status = \"âœ“ PASS\" if passed_test else \"âœ— FAIL\"\n",
    "        print(f\"  {criterion:20s}: {status}\")\n",
    "        if passed_test:\n",
    "            passed += 1\n",
    "    \n",
    "    print(f\"\\nOverall: {passed}/{total} criteria passed\")\n",
    "    \n",
    "    # Comparison to baselines\n",
    "    print(\"\\nğŸ“ˆ COMPARISON TO BASELINES:\")\n",
    "    baseline_means = {k: np.mean(v) for k, v in baseline_results.items()}\n",
    "    \n",
    "    print(f\"  Neural Operator:       RÂ² = {r2:.4f}\")\n",
    "    print(f\"  Last Value Forward:    RÂ² = {baseline_means['last_value']:.4f}\")\n",
    "    print(f\"  Linear Extrapolation:  RÂ² = {baseline_means['linear_extrap']:.4f}\")\n",
    "    print(f\"  Mean Trajectory:       RÂ² = {baseline_means['mean_trajectory']:.4f}\")\n",
    "    print(f\"  Random Forest (same-time): RÂ² = {rf_r2:.4f}\")\n",
    "    \n",
    "    # Final verdict\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    if passed >= 3:\n",
    "        print(\"ğŸ‰ SUCCESS! Model meets success criteria for trajectory prediction\")\n",
    "        print(\"\\nNext Steps:\")\n",
    "        print(\"  1. Deploy for real-time predictions\")\n",
    "        print(\"  2. Test on new strains (transfer learning)\")\n",
    "        print(\"  3. Add uncertainty quantification\")\n",
    "        print(\"  4. Integrate into wet lab workflow\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Model needs improvement\")\n",
    "        print(\"\\nRecommendations:\")\n",
    "        if not criteria['positive_r2']:\n",
    "            print(\"  â€¢ RÂ² is negative - model not learning properly\")\n",
    "            print(\"    â†’ Check data quality, normalization, trajectory alignment\")\n",
    "        if not criteria['beats_linear']:\n",
    "            print(\"  â€¢ Can't beat linear extrapolation\")\n",
    "            print(\"    â†’ Try simpler model architecture\")\n",
    "            print(\"    â†’ Add more training data\")\n",
    "        if not criteria['reasonable_rmse']:\n",
    "            print(\"  â€¢ RMSE too high\")\n",
    "            print(\"    â†’ Add regularization\")\n",
    "            print(\"    â†’ Balance dataset (undersample high producers?)\")\n",
    "    \n",
    "    return {\n",
    "        'r2': r2,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'criteria_passed': passed,\n",
    "        'criteria_total': total,\n",
    "        'success': passed >= 3\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 5: Visualization of Success\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_success_criteria(model, X_val, y_val, masks_val, sequences, \n",
    "                               val_idx, baseline_results, success_metrics):\n",
    "    \"\"\"\n",
    "    Create a single \"success dashboard\" figure.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_val).numpy()\n",
    "    \n",
    "    # Panel 1: Success criteria checklist\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    criteria_names = ['Positive RÂ²', 'Beats Last-Value', 'Beats Linear', \n",
    "                     'RMSE < 0.2', 'MAE < 0.15']\n",
    "    criteria_status = [1 if success_metrics['criteria_passed'] >= i else 0 \n",
    "                      for i in range(1, 6)]\n",
    "    \n",
    "    colors = ['green' if s else 'red' for s in criteria_status]\n",
    "    ax1.barh(criteria_names, [1]*len(criteria_names), color=colors, alpha=0.6, edgecolor='black')\n",
    "    ax1.set_xlim([0, 1])\n",
    "    ax1.set_xlabel('Pass/Fail', fontweight='bold')\n",
    "    ax1.set_title('Success Criteria', fontweight='bold', fontsize=12)\n",
    "    ax1.set_xticks([])\n",
    "    \n",
    "    # Panel 2: Baseline comparison\n",
    "    ax2 = fig.add_subplot(gs[0, 1:])\n",
    "    methods = ['Last Value', 'Linear Extrap', 'Mean Traj', 'Neural Operator']\n",
    "    r2_scores = [\n",
    "        np.mean(baseline_results['last_value']),\n",
    "        np.mean(baseline_results['linear_extrap']),\n",
    "        np.mean(baseline_results['mean_trajectory']),\n",
    "        success_metrics['r2']\n",
    "    ]\n",
    "    colors = ['lightcoral', 'lightsalmon', 'lightblue', 'lightgreen']\n",
    "    \n",
    "    bars = ax2.bar(methods, r2_scores, color=colors, edgecolor='black', linewidth=2)\n",
    "    ax2.set_ylabel('RÂ² Score', fontweight='bold')\n",
    "    ax2.set_title('Model Comparison (Trajectory Prediction)', fontweight='bold', fontsize=12)\n",
    "    ax2.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, score in zip(bars, r2_scores):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{score:.3f}', ha='center', fontweight='bold')\n",
    "    \n",
    "    # Panel 3-5: Example trajectories\n",
    "    example_indices = np.linspace(0, len(X_val)-1, 3, dtype=int)\n",
    "    \n",
    "    for i, val_seq_idx in enumerate(example_indices):\n",
    "        ax = fig.add_subplot(gs[1, i])\n",
    "        \n",
    "        original_seq_idx = val_idx[val_seq_idx]\n",
    "        seq = sequences[original_seq_idx]\n",
    "        \n",
    "        mask = masks_val.numpy()[val_seq_idx] > 0\n",
    "        n_valid = mask.sum()\n",
    "        \n",
    "        actual_time = seq['time'][:n_valid]\n",
    "        actual_psilocybin = seq['psilocybin'][:n_valid]\n",
    "        predicted_psilocybin = predictions[val_seq_idx][:n_valid]\n",
    "        \n",
    "        early_cutoff = 48\n",
    "        cutoff_idx = np.where(actual_time <= early_cutoff)[0][-1] if any(actual_time <= early_cutoff) else 1\n",
    "        \n",
    "        ax.scatter(actual_time[:cutoff_idx+1], actual_psilocybin[:cutoff_idx+1], \n",
    "                  color='green', s=60, marker='o', zorder=5, label='Early Data')\n",
    "        ax.scatter(actual_time[cutoff_idx+1:], actual_psilocybin[cutoff_idx+1:], \n",
    "                  color='red', s=60, marker='s', zorder=5, label='Future')\n",
    "        ax.plot(actual_time, predicted_psilocybin, 'b-', linewidth=2.5, alpha=0.7, label='Prediction')\n",
    "        \n",
    "        ax.set_xlabel('Time (h)', fontweight='bold')\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Psilocybin (g/L)', fontweight='bold')\n",
    "        ax.set_title(f'{seq[\"strain\"]}', fontweight='bold')\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Panel 6: Predicted vs Actual\n",
    "    ax6 = fig.add_subplot(gs[2, :2])\n",
    "    valid_pred = predictions[masks_val.numpy() > 0].flatten()\n",
    "    valid_true = y_val.numpy()[masks_val.numpy() > 0].flatten()\n",
    "    \n",
    "    ax6.scatter(valid_true, valid_pred, alpha=0.5, s=30, edgecolors='black', linewidths=0.5)\n",
    "    max_val = max(valid_true.max(), valid_pred.max())\n",
    "    ax6.plot([0, max_val], [0, max_val], 'r--', linewidth=2, label='Perfect')\n",
    "    \n",
    "    ax6.set_xlabel('Actual Psilocybin (g/L)', fontweight='bold')\n",
    "    ax6.set_ylabel('Predicted Psilocybin (g/L)', fontweight='bold')\n",
    "    ax6.set_title(f\"Overall: RÂ²={success_metrics['r2']:.3f}, RMSE={success_metrics['rmse']:.4f} g/L\", \n",
    "                 fontweight='bold')\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Panel 7: Summary text\n",
    "    ax7 = fig.add_subplot(gs[2, 2])\n",
    "    ax7.axis('off')\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "SUMMARY\n",
    "{'='*25}\n",
    "\n",
    "Criteria Passed: {success_metrics['criteria_passed']}/{success_metrics['criteria_total']}\n",
    "\n",
    "Performance:\n",
    "  RÂ² = {success_metrics['r2']:.4f}\n",
    "  RMSE = {success_metrics['rmse']:.4f} g/L\n",
    "  MAE = {success_metrics['mae']:.4f} g/L\n",
    "\n",
    "Verdict:\n",
    "  {'âœ“ SUCCESS' if success_metrics['success'] else 'âœ— NEEDS WORK'}\n",
    "\n",
    "{'Model ready for deployment!' if success_metrics['success'] else 'Continue hyperparameter tuning'}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax7.text(0.1, 0.5, summary_text, transform=ax7.transAxes, fontsize=10,\n",
    "            verticalalignment='center', fontfamily='monospace',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "    \n",
    "    plt.suptitle('Neural Operator Success Dashboard', fontsize=16, fontweight='bold', y=0.995)\n",
    "    # plt.savefig('success_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(\"\\nâœ“ Success dashboard created\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 6: Complete Validation Pipeline\n",
    "# ============================================================================\n",
    "\n",
    "def complete_validation_pipeline(model, X_train, y_train, masks_train,\n",
    "                                X_val, y_val, masks_val, sequences, val_idx, rf_r2):\n",
    "    \"\"\"\n",
    "    Run complete validation: baselines â†’ hyperparameters â†’ success criteria\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPLETE VALIDATION PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Evaluate baselines\n",
    "    baseline_results = evaluate_baselines(sequences, early_cutoff=48)\n",
    "    \n",
    "    # Step 2: Hyperparameter search (optional - can skip if model already trained)\n",
    "    # best_config, hp_results = hyperparameter_search(X_train, y_train, masks_train,\n",
    "    #                                                  X_val, y_val, masks_val)\n",
    "    \n",
    "    # Step 3: Evaluate success criteria\n",
    "    success_metrics = evaluate_success_criteria(model, X_val, y_val, masks_val, \n",
    "                                               sequences, baseline_results, rf_r2)\n",
    "    \n",
    "    # Step 4: Visualize\n",
    "    visualize_success_criteria(model, X_val, y_val, masks_val, sequences, \n",
    "                              val_idx, baseline_results, success_metrics)\n",
    "    \n",
    "    return success_metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\"\"\n",
    "    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "    â•‘        HYPERPARAMETER TUNING & SUCCESS VALIDATION SUITE              â•‘\n",
    "    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \n",
    "    This module provides:\n",
    "    1. Baseline predictors (last-value, linear extrapolation)\n",
    "    2. Hyperparameter search grid\n",
    "    3. Leave-one-strain-out cross-validation\n",
    "    4. Success criteria evaluation\n",
    "    5. Comprehensive visualization\n",
    "    \n",
    "    Usage:\n",
    "    ------\n",
    "    from hyperparameter_tuning import complete_validation_pipeline\n",
    "    \n",
    "    success_metrics = complete_validation_pipeline(\n",
    "        model, X_train, y_train, masks_train,\n",
    "        X_val, y_val, masks_val, sequences, val_idx, rf_r2\n",
    "    )\n",
    "    \n",
    "    if success_metrics['success']:\n",
    "        print(\"ğŸ‰ Model validated - ready for deployment!\")\n",
    "    else:\n",
    "        print(\"Continue tuning...\")\n",
    "    \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
